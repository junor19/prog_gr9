{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f3e1f3",
   "metadata": {},
   "source": [
    "Oppgaver: \n",
    "\n",
    "I dette prosjektet skal vi gjennomføre oppgaver som fokuserer på oppsett av utviklingsmiljø, innsamling, behandling, analyse, visualisering og prediktiv analyse av miljødata. Oppgave 1 handler om å sette opp et fungerende utviklingsmiljø. Oppgave 2 innebærer å identifisere relevante åpne datakilder og implementere funksjonalitet for å hente data fra disse kildene ved hjelp av Python. I oppgave 3 skal vi utvikle funksjoner for å rense og formatere de innsamlede dataene, med fokus på håndtering av manglende verdier.\n",
    "\n",
    "#### Oppgave 1: Sett opp utviklingsmiljø\n",
    "\n",
    "I VSCode, opprett en ny Jupyter Notebook-fil (med filendelsen «.ipynb») i prosjektmappen. Skriv og kjør kode i den første cellen for å teste at miljøet fungerer som det skal:\n",
    "\n",
    "Gå til [oppgave 1](oppg1.ipynb) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d9c533",
   "metadata": {},
   "source": [
    "#### Oppgave 2: Datainnsamling\n",
    "\n",
    "### Hvorfor MET?\n",
    "For dette prosjektet ønsket vi å se på litt ukonvesjonelle data, så vi valgte derfor å se på solskinn varighet. Solskinn kan indikere endringer i skydekke og værmønstre. Det kan også fortelle oss noe om hvor mye solenergi som når bakken som vil påvirke lokal tempratur og som videre kan spille inn på oppvarmingstrender. Spesielt utvikling over tid vil være interessant. \n",
    "\n",
    "Før vi bestemte oss for å undersøke sollys var vi først inne å gjode oss kjent med ulike API grensesnitt. Den som sto fram som et godt valg var 'The MET weather API', mer spesefikt valgte vi Frost API-en deres. Denne gir gratis tilgang til deres arkiv over historiske vær- og klimadata av stor variasjon og med mye detaljer av kvalitetskontrollerte målinger. \n",
    "\n",
    "Les mer her [Hva er Frost?](https://frost.met.no/index.html) og [Om Meteorologisk Institutt](https://www.met.no/en/About-us/About-MET-Norway)\n",
    "\n",
    "De har en fri og åpen data politikk, med ønske om at dataen de samler kan bli brukt fritt til fordel for samfunnet. Deres kontaktinformasjon, til og med en egen nettside [hvordan komme hit](https://www.met.no/en/contact-us/travel-here) er et godt tegn på legitimitet ettersom de er lett å komme i kontakt med. Det er et statlig etat, med formål om å informere med veldokumentert forsking samt nøytralt og saklig språk. Nettsiden er utformet på en overskitlig og profesjonell måte med mange forklaringer, oppslagsverk og dataklarifikasjoner som gjør forståelse og henting av data letter. \n",
    "\n",
    "### Hvorfor CSV-fil?\n",
    "\n",
    "Frost API bruker JSON-format til data, vi valgte å gjør om og lagre i CSV-fil ettersom det er standard for de fleste analyseverktøy og fungerer for eksempel bra med Pandas, Matplotlib og Plotly. Det er også raskt å laste inn og veldig lesbart. \n",
    "\n",
    "Dataen er strukturert slik:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0471092c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       date  sunshine_hours\n",
      "0  2016-01-01T00:00:00.000Z             0.9\n",
      "1  2016-01-02T00:00:00.000Z             1.3\n",
      "2  2016-01-03T00:00:00.000Z             3.5\n",
      "3  2016-01-04T00:00:00.000Z             3.6\n",
      "4  2016-01-05T00:00:00.000Z             2.8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/raa_data_d.csv\", nrows=5)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293ab326",
   "metadata": {},
   "source": [
    "# slett, vurderingskreterier fra mappe:\n",
    "I prosjektet for utvikling av en miljødataanalyseapplikasjon skal dere først identifisere relevante åpne datakilder, som f.eks. API-er fra meteorologiske institutter og miljøovervåkingsorganisasjoner. Deretter skal dere implementere funksjonalitet for å hente data fra disse kildene ved hjelp av Python-moduler (som eks. requests). For å integrere dataene i applikasjonen, bruker dere teknikker som håndtering av tekstfiler, CSV-filer, JSON-data, samt fil- og katalogadministrasjon. I tillegg skal dere benytte dere av list comprehensions, iteratorer og Pandas SQL (sqldf) for å utforske og forstå dataenes struktur og innhold før de forberedes for videre analyse.\n",
    "\n",
    "Vurderingskriterier:\n",
    "\n",
    "Hvilke åpne datakilder er identifisert som relevante for miljødata, og hva er kriteriene (f.eks. kildeautoritet, datakvalitet, tilgjengelighet, brukervennlighet osv.) for å vurdere deres pålitelighet og kvalitet?\n",
    "\n",
    "Hvilke teknikker (f.eks. håndtering av CSV-filer, JSON-data) er valgt å bruke for å lese inn dataene, og hvordan påvirker disse valgene datakvaliteten og prosessen videre?\n",
    "\n",
    "Dersom det er brukt API-er, hvilke spesifikke API-er er valgt å bruke, og hva er de viktigste dataene som kan hentes fra disse kildene?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afe9dc2",
   "metadata": {},
   "source": [
    "#### Oppgave 3: Databehandling\n",
    "\n",
    "Her skal vi fokusere på databehandling ved å utvikle funksjoner som renser og formaterer de innsamlede dataene, med særlig vekt på håndtering av manglende verdier og uregelmessigheter ved hjelp av Pandas. I tillegg skal vi benytte teknikker som list comprehensions, iteratorer, pandas og pandas sql (sqldf) for å manipulere dataene effektivt, noe som vil bidra til å forberede dataene for videre analyse.\n",
    "\n",
    "### hvilke endringer har vi gjort?\n",
    "\n",
    "Datasettet vi hentet fra MET var av høy kvalitet uten hull så vi valgte å fjerne data for å igjen kunne gjennopprette disse dataene med estimasjon.\n",
    "\n",
    "Her er en oversikt over dataen som ble fjernet og duplisert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a09d50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antall totale rader i datasettet: 1819\n",
      "antall dupliserte datoer: 8\n",
      "antall datoer som mangler: 16\n",
      "\n",
      "Dupliserte datoer:\n",
      "799   2018-03-19 00:00:00+00:00\n",
      "801   2018-03-20 00:00:00+00:00\n",
      "803   2018-03-21 00:00:00+00:00\n",
      "805   2018-03-22 00:00:00+00:00\n",
      "807   2018-03-23 00:00:00+00:00\n",
      "809   2018-03-24 00:00:00+00:00\n",
      "811   2018-03-25 00:00:00+00:00\n",
      "813   2018-03-26 00:00:00+00:00\n",
      "Name: date, dtype: datetime64[ns, UTC]\n",
      "\n",
      "Manglende datoer:\n",
      "['2016-10-01', '2016-10-02', '2016-10-03', '2017-07-18', '2017-07-19', '2017-07-20', '2017-07-21', '2017-07-22', '2017-07-23', '2017-07-24', '2019-12-25', '2019-12-26', '2019-12-27', '2019-12-28', '2019-12-29', '2019-12-30']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "#henter funksjonen finne_hull fra py-filen finne_mangler fra mappen src\n",
    "from src.finne_mangler import finne_hull\n",
    "\n",
    "filbane = \"../data/raa_data_d.csv\"\n",
    "duplikater, hull = finne_hull(filbane, dato_kolonne=\"date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecc369a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antall totale rader i datasettet: 1819\n",
      "antall dupliserte datoer: 8\n",
      "antall datoer som mangler: 16\n",
      "\n",
      "Dupliserte datoer:\n",
      "799   2018-03-19 00:00:00+00:00\n",
      "801   2018-03-20 00:00:00+00:00\n",
      "803   2018-03-21 00:00:00+00:00\n",
      "805   2018-03-22 00:00:00+00:00\n",
      "807   2018-03-23 00:00:00+00:00\n",
      "809   2018-03-24 00:00:00+00:00\n",
      "811   2018-03-25 00:00:00+00:00\n",
      "813   2018-03-26 00:00:00+00:00\n",
      "Name: date, dtype: datetime64[ns, UTC]\n",
      "\n",
      "Manglende datoer:\n",
      "['2016-10-01', '2016-10-02', '2016-10-03', '2017-07-18', '2017-07-19', '2017-07-20', '2017-07-21', '2017-07-22', '2017-07-23', '2017-07-24', '2019-12-25', '2019-12-26', '2019-12-27', '2019-12-28', '2019-12-29', '2019-12-30']\n",
      "\n",
      "Benyttet lineær interpolasjon, fil lagret som : utfylt_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\junoh\\Documents\\1 DigFor\\2 semester\\4114 anvendt programering\\prog_gr9\\proj_environment-main\\src\\finne_mangler.py:62: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_utfylt = pd.concat([df, df_manglende], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sunshine_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01 00:00:00+00:00</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-02 00:00:00+00:00</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-03 00:00:00+00:00</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-04 00:00:00+00:00</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-05 00:00:00+00:00</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>2020-12-27 00:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>2020-12-28 00:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>2020-12-29 00:00:00+00:00</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>2020-12-30 00:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>2020-12-31 00:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1827 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          date  sunshine_hours\n",
       "0    2016-01-01 00:00:00+00:00             0.9\n",
       "1    2016-01-02 00:00:00+00:00             1.3\n",
       "2    2016-01-03 00:00:00+00:00             3.5\n",
       "3    2016-01-04 00:00:00+00:00             3.6\n",
       "4    2016-01-05 00:00:00+00:00             2.8\n",
       "...                        ...             ...\n",
       "1822 2020-12-27 00:00:00+00:00             0.0\n",
       "1823 2020-12-28 00:00:00+00:00             0.0\n",
       "1824 2020-12-29 00:00:00+00:00             0.6\n",
       "1825 2020-12-30 00:00:00+00:00             0.0\n",
       "1826 2020-12-31 00:00:00+00:00             0.0\n",
       "\n",
       "[1827 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.finne_mangler import vasket_data\n",
    "vasket_data(\"../data/raa_data_d.csv\", dato_kolonne=\"date\", output_fil=\"utfylt_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54acb50b",
   "metadata": {},
   "source": [
    "### Håndtering av  uregelmessigheter i dataen\n",
    "\n",
    "Manglene og uregelmessighetene vi forventet å møte var:\n",
    "1. manglende datoer\n",
    "2. duplikerte datoer\n",
    "3. manglende verider i målingene (NaN)\n",
    "4. feilformaterte datoer\n",
    "\n",
    "For å håndtere 1. feil lagde vi funksjonene finne_hull(), mer spesifikt brukte vi pandas pd.date_range() for å finne hvilke datoer det gjelder for å så fyller hullene ved metoden interpolasjon i funksjonen vasket_data().\n",
    "\n",
    "for å fjerne duplikater datoer brukte vi drop_duplicates()\n",
    "\n",
    "Manglende målinger blir plukket opp ved hjelp av en løkke hvor alle verdiene får en type middlertidig \"missing value\" for senere å interpolere dem.\n",
    "\n",
    "for datoer som kan være i feil format eller tekst brukte vi pd.to_datetime() for å standarisere formatet. \n",
    "\n",
    "\n",
    "### Pandas SQL\n",
    "\n",
    "Med sqldf er det mulig å bruke SQL-spørringer på Pandas DataFrames. Altså kan man gjennomføre komplekse joins, grupperinger og filtreretinger på en veldig lesbar måte. Dette vil være en intuitiv måte å jobbe med dataen på om man er kjent med SQL fra før.\n",
    "\n",
    "les mer her [using SQL with Pandas DataFrames](https://medium.com/@davidfagb/using-sql-with-pandas-dataframes-1c36f57ea65d)\n",
    "\n",
    "### List comperhensions\n",
    "En måte å manipulere data er ved hjelp av list comprehensions, for eksempel konvertere vi DateTimeIndex-en fra finne_hull() som gjorde datoene mer lesbar som 'YYYY-MM-DD'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc4382e",
   "metadata": {},
   "source": [
    "## Slett, vurderingskriterie fra mappe: Oppgave 3: Databehandling\n",
    "Her skal dere fokusere på databehandling ved å utvikle funksjoner som renser og formaterer de innsamlede dataene, med særlig vekt på håndtering av manglende verdier og uregelmessigheter ved hjelp av Pandas. I tillegg skal dere benytte teknikker som list comprehensions, iteratorer, pandas og pandas sql (sqldf) for å manipulere dataene effektivt, noe som vil bidra til å forberede dataene for videre analyse.\n",
    "\n",
    "Vurderingskriterier:\n",
    "\n",
    "Hvilke metoder vil du bruke for å identifisere og håndtere manglende verdier i datasettet?\n",
    "\n",
    "Kan du gi et eksempel på hvordan du vil bruke list comprehensions for å manipulere dataene?\n",
    "\n",
    "Hvordan kan Pandas SQL (sqldf) forbedre datamanipuleringen sammenlignet med tradisjonelle Pandas-operasjoner?\n",
    "\n",
    "Hvilke spesifikke uregelmessigheter i dataene forventer du å møte, og hvordan planlegger du å håndtere dem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b8beb",
   "metadata": {},
   "source": [
    "Gå tilbake til [← innledningen](miljoanalyseprosjekt.ipynb) eller [del 2 →](del2.ipynb) av prosjektet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
